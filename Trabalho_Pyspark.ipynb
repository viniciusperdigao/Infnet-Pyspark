{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas e Libs"
      ],
      "metadata": {
        "id": "fyzZVRf1eWqZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hCNc589dioj",
        "outputId": "b5498863-9617-4a6e-f729-861f92add540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=33022de5c57893d6a678bb3f59be6d16612fc58b313e24aa79de7f3c230f8f54\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar as bibliotecas necessárias\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8rGiL91IeazS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma sessão Spark\n",
        "spark = SparkSession.builder.appName(\"Atividade01\").getOrCreate()"
      ],
      "metadata": {
        "id": "vzb_BRqaePvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1"
      ],
      "metadata": {
        "id": "YCrLAP7Udu3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenvolva um notebook bem documentado em Python utilizando a biblioteca PySpark para apresentar as análises realizadas no trabalho do Hadoop"
      ],
      "metadata": {
        "id": "48Ibwyrsd0U3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada análise realizada, escreva a consulta em SQL no Spark e também utilize as funções vistas em aula\n",
        "Acrescente novas análises para uma análise exploratória mais detalhada\n",
        "Utilize funções de agregação (count, min, max, avg, sum, etc.)"
      ],
      "metadata": {
        "id": "I_O5fEqed5vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) A Diretora Camilla Prado solicitou uma pesquisa que informe todas as obras cadastradas no acervo ordenadas por data de publicação."
      ],
      "metadata": {
        "id": "vVn76PdwhK_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler fontes de dados\n",
        "\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "editoras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/editora_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "autores_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/autor_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "spark_df = spark.createDataFrame(pd_df)\n",
        "\n",
        "\n",
        "# Converter de pandas para um dataframe spark.\n",
        "obras = spark.createDataFrame(obras_pd)\n",
        "editoras = spark.createDataFrame(editoras_pd)\n",
        "autores = spark.createDataFrame(autores_pd)\n",
        "\n",
        "\n",
        "# Realizar o join das tabelas\n",
        "obras_editoras = obras.join(editoras, on=[\"ID_Editora\"], how=\"inner\")\n",
        "obras_autores = obras_editoras.join(autores, on=[\"ID_Autor\"], how=\"inner\")\n",
        "\n",
        "# Selecionar as colunas desejadas\n",
        "obras_autores_editoras = obras_autores.select(\n",
        "    F.col(\"Titulo_Obra\").alias(\"Obra\"),\n",
        "    F.col(\"Nome_Autor\").alias(\"Autor\"),\n",
        "    F.col(\"Nome_Editora\").alias(\"Editora\"),\n",
        "    F.col(\"Data_Publicação\").alias(\"Data de Publicação\"),\n",
        ")\n",
        "\n",
        "# Ordenar os dados pela data de publicação\n",
        "obras_autores_editoras = obras_autores_editoras.orderBy(\"Data de Publicação\")\n",
        "\n",
        "# Exibir os resultados\n",
        "obras_autores_editoras.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibffVocafxKC",
        "outputId": "aa79a58d-714a-4ae5-a809-696435401fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------+------------------+\n",
            "|                Obra|               Autor|Editora|Data de Publicação|\n",
            "+--------------------+--------------------+-------+------------------+\n",
            "| Fortaleza de Sharpe|     Alexandre Dumas|   Casa|        01/01/2001|\n",
            "|      Arte Da Guerra|George Raymond Ri...|Saraiva|        01/05/1913|\n",
            "|    Filhos e Amantes|       Alberto Mussa|   Leya|        01/09/2005|\n",
            "|     Falcão de Malta|   Casimiro de Abreu|Saraiva|        02/01/2010|\n",
            "|      Finis Patriae |     Ariano Suassuna|   Casa|        04/03/2013|\n",
            "|Triste Fim de Pol...|George Raymond Ri...|   Casa|        05/03/2001|\n",
            "|Os Três Mosqueteiros|     Alexandre Dumas|  Aleph|        06/03/1953|\n",
            "|   Diabo dos Números| Clive Staples Lewis|   Nova|        06/05/2013|\n",
            "|      Furia dos Reis|George Raymond Ri...|   Leya|        06/08/2008|\n",
            "|  Discurso do Método|     Ariano Suassuna|Moderna|        06/08/2012|\n",
            "|         Vidas Secas|        Rick Riordan|Moderna|        09/07/2004|\n",
            "|O Conde de Monte ...|     Alexandre Dumas|  Aleph|        10/05/2005|\n",
            "|  Dia dos Gafanhotos|   Clarice Lispector|  Ática|        11/11/2011|\n",
            "|        Filho Nativo|     Alexandre Dumas|   Leya|        12/06/2004|\n",
            "|  Deixados para Trás|      Autran Dourado|  Draco|        12/09/1931|\n",
            "|       Frankenstein |     Denis Mandarino|  Aleph|        13/12/2010|\n",
            "|Tratado da Nature...|        Rick Riordan|   Nova|        14/05/2002|\n",
            "|          Rei Arthur|John Ronald Reuel...|  Draco|        15/07/1999|\n",
            "|   Flores sem Fruto |  Aluísio de Azevedo|  Ática|        15/08/2001|\n",
            "|        Dom Casmurro|   Casimiro de Abreu|   Leya|        15/08/2011|\n",
            "+--------------------+--------------------+-------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2)  O Governador vai doar duzentos livros para a Biblioteca, mas só irá doar se a biblioteca tiver menos de 300 obras. O Gerente Márcio Tales solicitou que fosse feita a contagem de quantas obras a Biblioteca possui atualmente."
      ],
      "metadata": {
        "id": "tyAmtKhVhh15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler os dados da tabela\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "obras = spark.createDataFrame(obras_pd)\n",
        "# Contar o número de títulos de obras distintos\n",
        "total_obras = obras.select(F.countDistinct(\"Titulo_Obra\").alias(\"Total de Obras\"))\n",
        "\n",
        "# Exibir os resultados\n",
        "total_obras.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdnOEicohlDl",
        "outputId": "92256f8b-902c-4c2a-d3b6-5d08ca937424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|Total de Obras|\n",
            "+--------------+\n",
            "|            29|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) A Gerência solicitou uma pesquisa para saber quais datas ocorreram empréstimos de livros e a quantidade emprestada. A consulta deverá retornar apenas um registro para cada data."
      ],
      "metadata": {
        "id": "zO9eSAGah5jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler os dados da tabela\n",
        "emprestimos_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/emprestimo_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "emprestimos = spark.createDataFrame(emprestimos_pd)\n",
        "# Agrupar os empréstimos pela data de empréstimo\n",
        "emprestimos_agrupados = emprestimos.groupBy(\"Data_Emprestimo\")\n",
        "\n",
        "# Contar o número de empréstimos por data\n",
        "total_emprestimos = emprestimos_agrupados.agg(F.count(\"id_emprestimo\").alias(\"Total_de_Emprestimos\"))\n",
        "\n",
        "# Ordenar os resultados pelo número de empréstimos em ordem decrescente\n",
        "total_emprestimos = total_emprestimos.orderBy(\"Total_de_Emprestimos\", ascending=False)\n",
        "\n",
        "# Exibir os resultados\n",
        "total_emprestimos.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoQkH9h6h7NG",
        "outputId": "e8d3b80b-4c04-41f7-fd94-310062e3c672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+\n",
            "|Data_Emprestimo|Total_de_Emprestimos|\n",
            "+---------------+--------------------+\n",
            "|     07/07/2013|                   4|\n",
            "|     20/08/2011|                   3|\n",
            "|     19/08/2011|                   3|\n",
            "|     25/06/2013|                   3|\n",
            "|     31/01/2013|                   3|\n",
            "|     28/12/2012|                   3|\n",
            "|     18/08/2011|                   1|\n",
            "|     11/07/2011|                   1|\n",
            "|     01/04/2012|                   1|\n",
            "|     25/03/2012|                   1|\n",
            "|     29/03/2012|                   1|\n",
            "|     31/03/2012|                   1|\n",
            "|     27/03/2012|                   1|\n",
            "|     15/08/2011|                   1|\n",
            "|     09/06/2011|                   1|\n",
            "|     26/09/2011|                   1|\n",
            "+---------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) O Funcionário João Paulo Assistente de RH solicitou uma pesquisa que informasse todos os empréstimos que a Recepcionista Alice Meire fez no horário das 8hs as 9hs.\n"
      ],
      "metadata": {
        "id": "g2IAdLnLiTA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler os dados das tabelas\n",
        "emprestimos_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/emprestimo_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "emprestimos = spark.createDataFrame(emprestimos_pd)\n",
        "\n",
        "funcionarios_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/funcionario_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "funcionarios = spark.createDataFrame(funcionarios_pd)\n",
        "\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "obras = spark.createDataFrame(obras_pd)\n",
        "\n",
        "\n",
        "# Realizar o join das tabelas\n",
        "emprestimos_funcionarios = emprestimos.join(funcionarios, on=[\"ID_Funcionario\"], how=\"inner\")\n",
        "emprestimos_obras = emprestimos_funcionarios.join(obras, on=[\"ID_Obra\"], how=\"inner\")\n",
        "\n",
        "# Selecionar as colunas desejadas\n",
        "emprestimos_filtrados = emprestimos_obras.select(\n",
        "    F.col(\"Nome_Funcionario\").alias(\"Nome do Funcionario\"),\n",
        "    F.col(\"Titulo_Obra\").alias(\"Titulo da Obra\"),\n",
        "    F.col(\"Hora_Emprestimo\").alias(\"Horário do Empréstimo\"),\n",
        ")\n",
        "\n",
        "# Filtrar os empréstimos realizados pelo funcionário 8\n",
        "emprestimos_filtrados = emprestimos_filtrados.filter(emprestimos_filtrados[\"Nome do Funcionario\"] == \"Alice Meire \")\n",
        "\n",
        "# Filtrar os empréstimos realizados entre 8h00 e 9h00\n",
        "emprestimos_filtrados = emprestimos_filtrados.filter(\n",
        "    (emprestimos_filtrados[\"Horário do Empréstimo\"] >= \"8:00\") & (\n",
        "        emprestimos_filtrados[\"Horário do Empréstimo\"] <= \"9:00\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Exibir os resultados\n",
        "emprestimos_filtrados.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXQwBawWiVSu",
        "outputId": "f5f26728-1be9-4350-e491-f4def5c406bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------+---------------------+\n",
            "|Nome do Funcionario|  Titulo da Obra|Horário do Empréstimo|\n",
            "+-------------------+----------------+---------------------+\n",
            "|       Alice Meire | Falcão de Malta|                 8:00|\n",
            "|       Alice Meire |Força das Coisas|                 8:36|\n",
            "|       Alice Meire |   Frankenstein |                 8:00|\n",
            "+-------------------+----------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) - A Diretoria solicitou uma pesquisa sobre devolução de livros entre as datas 29/03/2012 a 02/02/2013."
      ],
      "metadata": {
        "id": "98vawtEhkZCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler os DataFrames das tabelas dw_devolucao, dw_usuario e dw_obra\n",
        "devolucao_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/devolucao_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_devolucao = spark.createDataFrame(devolucao_pd)\n",
        "\n",
        "usuario_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/usuario_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_usuario = spark.createDataFrame(usuario_pd)\n",
        "\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_obra = spark.createDataFrame(obras_pd)\n",
        "\n",
        "\n",
        "# Realizar um join entre as tabelas dw_devolucao, dw_usuario e dw_obra\n",
        "dw_devolucao_joined = dw_devolucao.join(dw_usuario, dw_devolucao[\"ID_Usuario\"] == dw_usuario[\"ID_Usuario\"], \"inner\")\n",
        "dw_devolucao_joined = dw_devolucao_joined.join(dw_obra, dw_devolucao_joined[\"ID_Obra\"] == dw_obra[\"ID_Obra\"], \"inner\")\n",
        "\n",
        "# Selecionar as colunas desejadas\n",
        "dw_devolucao_filtered = dw_devolucao_joined.select(\n",
        "    F.col(\"Nome_Usuario\").alias(\"Nome Usuário\"),\n",
        "    F.col(\"Titulo_Obra\").alias(\"Titulo da Obra\"),\n",
        "    F.col(\"Data_Devolucao\"),\n",
        "    F.col(\"Hora_Devolucao\"),\n",
        "    F.col(\"Multa_Atraso\").alias(\"Multa por Atraso\"),\n",
        ")\n",
        "\n",
        "# Filtrar as datas de devolução\n",
        "dw_devolucao_filtered = dw_devolucao_filtered.filter(\n",
        "    F.col(\"Data_Devolucao\").between(\"29/03/2012\", \"02/02/2013\")\n",
        ")\n",
        "\n",
        "# Ordenar o DataFrame pela data de devolução\n",
        "dw_devolucao_ordered = dw_devolucao_filtered.orderBy(F.desc(\"Data_Devolucao\"))\n",
        "\n",
        "# Visualizar o DataFrame ordenado\n",
        "dw_devolucao_ordered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK235kTvkbC3",
        "outputId": "776a6166-1b55-44ed-9556-8ad6fc38e8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------+--------------+--------------+----------------+\n",
            "|Nome Usuário|Titulo da Obra|Data_Devolucao|Hora_Devolucao|Multa por Atraso|\n",
            "+------------+--------------+--------------+--------------+----------------+\n",
            "+------------+--------------+--------------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) - A Gerência solicitou uma pesquisa para saber quais reservas de livros que foram feitas com data maior  ou igual a 18/08/2011."
      ],
      "metadata": {
        "id": "Vf4GAKh8lvXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregue os DataFrames correspondentes às tabelas\n",
        "reserva_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/reserva_origem.csv\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_reserva = spark.createDataFrame(reserva_pd)\n",
        "\n",
        "usuario_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/usuario_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_usuario = spark.createDataFrame(usuario_pd)\n",
        "\n",
        "\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_obra = spark.createDataFrame(obras_pd)\n",
        "\n",
        "# Prefixe as colunas com o alias da tabela\n",
        "dw_reserva = dw_reserva.alias(\"reserva\")\n",
        "dw_usuario = dw_usuario.alias(\"usuario\")\n",
        "dw_obra = dw_obra.alias(\"obra\")\n",
        "\n",
        "# Realize as junções e selecione as colunas desejadas\n",
        "resultado = dw_reserva \\\n",
        "    .join(dw_usuario, F.col(\"reserva.ID_Usuario\") == F.col(\"usuario.ID_Usuario\")) \\\n",
        "    .join(dw_obra, F.col(\"reserva.ID_Obra\") == F.col(\"obra.ID_Obra\")) \\\n",
        "    .select(\n",
        "        F.col(\"obra.Titulo_Obra\").alias('Titulo da Obra'),\n",
        "        F.col(\"reserva.Status_Livro\").alias('Status da Reserva'),\n",
        "        F.col(\"reserva.Data_Reserva\").alias('Data da Reserva'),\n",
        "        F.col(\"reserva.Hora_Reserva\").alias('Hora da Reserva'),\n",
        "        F.col(\"usuario.Nome_Usuario\").alias('Reservado por')\n",
        "    )\n",
        "\n",
        "# Aplicar as condições do WHERE\n",
        "resultado = resultado.filter(\n",
        "    (F.col(\"Data da Reserva\") == '18/08/2011') & (resultado['Status da Reserva'] == 'Reservado')\n",
        ")\n",
        "\n",
        "# Mostrar o resultado\n",
        "resultado.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZMjITPylxIp",
        "outputId": "1eb07747-d21f-4ad1-b032-84e899b8747d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+---------------+---------------+-------------------+\n",
            "|   Titulo da Obra|Status da Reserva|Data da Reserva|Hora da Reserva|      Reservado por|\n",
            "+-----------------+-----------------+---------------+---------------+-------------------+\n",
            "|Diabo dos Números|        Reservado|     18/08/2011|          15:00|Juliana Bento Souza|\n",
            "|        Fluviais |        Reservado|     18/08/2011|          15:00|    Ernesto Coimbra|\n",
            "+-----------------+-----------------+---------------+---------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) -  A área de RH solicitou uma pesquisa para saber quais devoluções de livros  foram feitas antes de 29/03/2012."
      ],
      "metadata": {
        "id": "97_5ZPs4pMam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ler os DataFrames das tabelas dw_devolucao e dw_obra\n",
        "devolucao_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/devolucao_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_devolucao = spark.createDataFrame(devolucao_pd)\n",
        "\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_obra = spark.createDataFrame(obras_pd)\n",
        "\n",
        "\n",
        "# Realizar um join entre as tabelas dw_devolucao e dw_obra\n",
        "dw_devolucao_joined = dw_devolucao.join(dw_obra, \"ID_Obra\", \"inner\")\n",
        "\n",
        "# Selecionar as colunas desejadas\n",
        "dw_devolucao_filtered = dw_devolucao_joined.select(\n",
        "    F.col(\"Titulo_Obra\"),\n",
        "    F.col(\"Data_Devolucao\"),\n",
        "    F.col(\"Hora_Devolucao\"),\n",
        "    F.col(\"Multa_Atraso\"),\n",
        ")\n",
        "\n",
        "# Filtrar as datas de devolução\n",
        "dw_devolucao_filtered = dw_devolucao_filtered.filter(\n",
        "    F.col(\"Data_Devolucao\") < \"29/03/2012\"\n",
        ")\n",
        "\n",
        "# Visualizar o DataFrame filtrado\n",
        "dw_devolucao_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEyuVe_hqnYR",
        "outputId": "1fcb6733-4eb6-466c-e236-0291b99b7a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+--------------+------------+\n",
            "|         Titulo_Obra|Data_Devolucao|Hora_Devolucao|Multa_Atraso|\n",
            "+--------------------+--------------+--------------+------------+\n",
            "|O Conde de Monte ...|    03/04/2012|         10:00|           0|\n",
            "|Triste Fim de Pol...|    13/07/2011|         13:00|           0|\n",
            "|   Diabo dos Números|    21/08/2011|         14:55|           0|\n",
            "|   Diabo dos Números|    08/04/2012|         11:11|           1|\n",
            "|      Finis Patriae |    12/01/2013|         15:00|           1|\n",
            "|Os Três Mosqueteiros|    26/08/2011|         10:00|           1|\n",
            "|     Falcão de Malta|    11/06/2011|         18:00|           0|\n",
            "|     Falcão de Malta|    17/08/2011|         14:55|           0|\n",
            "|     Falcão de Malta|    28/09/2011|         14:29|           0|\n",
            "|   Flores sem Fruto |    21/08/2011|         10:00|           0|\n",
            "|           Fluviais |    20/08/2011|         12:01|           0|\n",
            "|    Força das Coisas|    22/08/2011|         18:00|           0|\n",
            "|       Frankenstein |    22/08/2011|         14:55|           0|\n",
            "|          Rei Arthur|    21/08/2011|         13:00|           0|\n",
            "|  Dia dos Gafanhotos|    02/02/2013|         18:00|           0|\n",
            "+--------------------+--------------+--------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) - A Gerência solicitou uma pesquisa para saber quais obras existem no acervo que são diferentes dos títulos das obras ‘O Conde de Monte Cristo’ e ‘Filhos e Amantes’  ."
      ],
      "metadata": {
        "id": "940lz1IkrIO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ler o DataFrame da tabela dw_obra\n",
        "obras_pd = pd.read_csv(\"https://raw.githubusercontent.com/viniciusperdigao/Infraestrutura-Hadoop-Infnet/main/bases/obra_origem.CSV\", encoding='latin1',header='infer',delimiter=';')\n",
        "dw_obra = spark.createDataFrame(obras_pd)\n",
        "\n",
        "\n",
        "# Selecionar as colunas desejadas\n",
        "dw_obra_filtered = dw_obra.select(\n",
        "    F.col(\"Titulo_Obra\"),\n",
        "    F.col(\"Gênero\"),\n",
        "    F.col(\"Data_Publicação\"),\n",
        ")\n",
        "\n",
        "# Filtrar as obras que não são \"O Conde de Monte Cristo\" ou \"Filhos e Amantes\"\n",
        "\n",
        "dw_obra_filtered = dw_obra_filtered.filter( (F.col(\"Titulo_Obra\") != \"O Conde de Monte Cristo\") & (F.col(\"Titulo_Obra\") != \"Filhos e Amantes\"))\n",
        "\n",
        "# Visualizar o DataFrame filtrado\n",
        "dw_obra_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW7XkawOrKMF",
        "outputId": "390e4ee8-0ccd-4dac-aef9-78cd09036e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+---------------+\n",
            "|         Titulo_Obra|     Gênero|Data_Publicação|\n",
            "+--------------------+-----------+---------------+\n",
            "|Tratado de Confissom|  Filosofia|     31/12/2010|\n",
            "|Triste Fim de Pol...|   Política|     05/03/2001|\n",
            "|Tratado da Nature...|    Romance|     14/05/2002|\n",
            "|Farsa de Inês Per...|  Religioso|     25/04/1986|\n",
            "|        Filho Nativo|      Poema|     12/06/2004|\n",
            "|     Jogo Dos Tronos|     Ficção|     26/08/2001|\n",
            "|   Diabo dos Números|     Terror|     31/08/1981|\n",
            "|      Furia dos Reis|Estrangeiro|     06/08/2008|\n",
            "|      Finis Patriae |   Política|     04/03/2013|\n",
            "|     Finnegans Wake |    Romance|     30/09/2013|\n",
            "|Os Três Mosqueteiros|    Romance|     06/03/1953|\n",
            "|     Falcão de Malta| Auto Ajuda|     02/01/2010|\n",
            "|         Vidas Secas|  Filosofia|     09/07/2004|\n",
            "|   Flores sem Fruto |   Política|     15/08/2001|\n",
            "|  Deixados para Trás|   Política|     12/09/1931|\n",
            "|     Deus das Moscas|    Romance|     25/09/2003|\n",
            "|    Senhor dos Aneis|     Ficção|     28/02/1989|\n",
            "|           Fluviais |     Terror|     18/03/2011|\n",
            "|      Folhas Caídas |     Ficção|     19/09/2010|\n",
            "|    Força das Coisas|     Terror|     25/11/1985|\n",
            "+--------------------+-----------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2"
      ],
      "metadata": {
        "id": "fZIym7PEu8Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenvolva um notebook bem documentado em Python utilizando a biblioteca PySpark para treinar um modelo de aprendizado de máquina"
      ],
      "metadata": {
        "id": "R0nrzl3Hu9Tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo de análise preditiva em dados de inspeção de alimentos"
      ],
      "metadata": {
        "id": "S7JVS_WYwKyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contexto: Usaremos o  Spark para executar uma análise preditiva sobre os dados de inspeção de alimentos (Food_Inspections1.csv). Dados adquiridos por meio do portal de dados da cidade de Chicago. Esse conjunto de dados contém informações sobre inspeções de estabelecimentos de alimentos realizadas em Chicago. Incluindo informações sobre cada estabelecimento de alimentos inspecionado, as violações encontradas (se houver) e os resultados da inspeção."
      ],
      "metadata": {
        "id": "FFd-arV2wMKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "5bE6AHNxwi9O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n"
      ],
      "metadata": {
        "id": "KZ_kQENE0jIV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar uma sessão Spark\n",
        "spark = SparkSession.builder.appName(\"Atividade02\").getOrCreate()"
      ],
      "metadata": {
        "id": "nUJ8oMTx0M7r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://raw.githubusercontent.com/jpvelez/restaurant_inspection_analysis/master/data/food_inspections.csv"
      ],
      "metadata": {
        "id": "530KBoR71mBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um dataframe com os dados do food inspections lendo uma base do github."
      ],
      "metadata": {
        "id": "WjcQzPiS32YA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd = pd.read_csv(\"https://raw.githubusercontent.com/jpvelez/restaurant_inspection_analysis/master/data/food_inspections.csv\", encoding='utf-8',header='infer')\n",
        "df = spark.createDataFrame(df_pd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8tA4D2e0xcb",
        "outputId": "109e3f31-cefe-441a-c283-8a61ccc1742f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ff60f51c67c8>:1: DtypeWarning: Columns (14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_pd = pd.read_csv(\"https://raw.githubusercontent.com/jpvelez/restaurant_inspection_analysis/master/data/food_inspections.csv\", encoding='utf-8',header='infer')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entendendo os dados"
      ],
      "metadata": {
        "id": "8lwP2ggq5DM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos começar a ter uma ideia do que o nosso conjunto de dados contém."
      ],
      "metadata": {
        "id": "L8k6afVn5EyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEs8PXM53hi9",
        "outputId": "e78e514b-50e3-4bc6-fd45-5cab396f41bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+--------------------+---------+------------------+---------------+--------------------+-------+-----+-------+---------------+--------------------+-------+--------------------+------------+------------+-----------+------------+--------------------+\n",
            "|Inspection ID|            DBA Name|            AKA Name|License #|     Facility Type|           Risk|             Address|   City|State|    Zip|Inspection Date|     Inspection Type|Results|          Violations|X Coordinate|Y Coordinate|   Latitude|   Longitude|            Location|\n",
            "+-------------+--------------------+--------------------+---------+------------------+---------------+--------------------+-------+-----+-------+---------------+--------------------+-------+--------------------+------------+------------+-----------+------------+--------------------+\n",
            "|      1229917|     ACAPULCO ALEGRE|     ACAPULCO ALEGRE|2162511.0|        Restaurant|  Risk 1 (High)|5306 W FULLERTON ...|CHICAGO|   IL|60639.0|     07/02/2012|             License|   Fail|18. NO EVIDENCE O...| 1140571.706| 1915485.097|41.92418126|-87.75890768|(41.9241812567116...|\n",
            "|      1114356|FIVE GUYS BURGER ...|FIVE GUYS BURGER ...|2163680.0|        Restaurant|Risk 2 (Medium)|1148 S WABASH AVE...|CHICAGO|   IL|60605.0|     07/02/2012|             License|   Pass|                 NaN| 1176915.605| 1895213.237|41.86780856|-87.62598003|(41.8678085563387...|\n",
            "|      1229907|LAVO ASIAN GRILL ...|    LAVO ASIAN GRILL|2152960.0|        Restaurant|  Risk 1 (High)|    6044 N BROADWAY |CHICAGO|   IL|60660.0|     07/02/2012|             License|   Fail|18. NO EVIDENCE O...| 1167161.894| 1940463.077|41.99219146|-87.66048377|(41.9921914595881...|\n",
            "|      1088215|LITTLE APPLE SEED...|LITTLE APPLE SEED...|2120438.0|Daycare Combo 1586|  Risk 1 (High)|  2671-73 E 75TH ST |CHICAGO|   IL|60649.0|     07/02/2012|             License|   Pass|41. PREMISES MAIN...| 1195537.389| 1855932.611| 41.7595793|-87.55891724|(41.7595792990767...|\n",
            "|      1229923| LONG MINI MART, INC| LONG MINI MART, INC|2163104.0|     Grocery Store|Risk 2 (Medium)|   5367 W NORTH AVE |CHICAGO|   IL|60639.0|     07/02/2012|License Re-Inspec...|   Pass|34. FLOORS: CONST...|  1140157.28| 1910069.858|41.90932885|-87.76056337|(41.9093288509131...|\n",
            "|      1114355|         PRAHEM INC.|              PRAHEM|2162730.0|     Grocery Store|   Risk 3 (Low)|   8 S MICHIGAN AVE |CHICAGO|   IL|60603.0|     07/02/2012|             License|   Fail|2. FACILITIES TO ...| 1177274.419|  1900351.28|41.88189952|-87.62450701|(41.8818995233182...|\n",
            "|      1084278|TAQUERIA EL TACOT...|TAQUERIA EL TACOTOTE|2163413.0|        Restaurant|  Risk 1 (High)|     1536 W 18TH ST |CHICAGO|   IL|60608.0|     07/02/2012|             License|   Pass|35. WALLS, CEILIN...| 1166257.798| 1891525.373|41.85792306|-87.66521187|(41.8579230575926...|\n",
            "|      1114353|THE GREAT STATE FARE|SANDELLA'S FLATBR...|  20481.0|        Restaurant|Risk 2 (Medium)|  100 W RANDOLPH ST |CHICAGO|   IL|60601.0|     07/02/2012|             Canvass|   Pass|                 NaN| 1175495.409| 1901315.768|41.88458627|-87.63101045|(41.8845862671545...|\n",
            "|      1229905|URBAN VEGAN LINCO...|         URBAN VEGAN|2158668.0|        Restaurant|  Risk 1 (High)|1550 W FULLERTON ...|CHICAGO|   IL|60614.0|     07/02/2012|License Re-Inspec...|   Pass|                 NaN| 1165364.274| 1916059.644|41.92526597|-87.66779312|(41.9252659719581...|\n",
            "|      1229902|               BAMBI|               BAMBI|  30171.0|         Wholesale|  Risk 1 (High)|     2051 W 47TH ST |CHICAGO|   IL|60609.0|     06/29/2012|             Canvass|   Pass|                 NaN| 1163260.387| 1873453.699|41.80839592|-87.67672122|(41.8083959179310...|\n",
            "+-------------+--------------------+--------------------+---------+------------------+---------------+--------------------+-------+-----+-------+---------------+--------------------+-------+--------------------+------------+------------+-----------+------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos checar valores distintos de resultados:"
      ],
      "metadata": {
        "id": "0gwqg2IJ5Ozr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('results').distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB5__f3s4Inw",
        "outputId": "35b95e93-f43b-4623-da82-8b1648aca008"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|             results|\n",
            "+--------------------+\n",
            "|                Fail|\n",
            "|            No Entry|\n",
            "|Business Not Located|\n",
            "|  Pass w/ Conditions|\n",
            "|     Out of Business|\n",
            "|                Pass|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos visualizar a distribuição desses resultados:"
      ],
      "metadata": {
        "id": "B8fgV6An5XMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('CountResults')\n",
        "resultado = spark.sql(\"SELECT COUNT(results) AS cnt, results FROM CountResults GROUP BY results\")\n",
        "resultado.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCDamE454M6V",
        "outputId": "5dd8ecea-e11d-43ef-c98f-83580d786b10"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|  cnt|             results|\n",
            "+-----+--------------------+\n",
            "|10624|                Fail|\n",
            "|   11|            No Entry|\n",
            "|   33|Business Not Located|\n",
            "| 3587|  Pass w/ Conditions|\n",
            "| 2777|     Out of Business|\n",
            "|28258|                Pass|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para prever um resultado de inspeção de alimentos, você precisa desenvolver um modelo com base nas violações. Como a regressão logística é um método de classificação binária, faz sentido agrupar os dados de resultado em duas categorias: Reprovado e Aprovado:\n",
        "\n",
        "* Aprovado\n",
        "\n",
        "  * Aprovado\n",
        "  * Aprovado c/ condições\n",
        "\n",
        "* Falha\n",
        "\n",
        "  * Falha\n",
        "\n",
        "* Descartar\n",
        "  * Negócios não localizados\n",
        "  * Fora de negócio\n",
        "  \n",
        "Dados com os outros resultados (“Negócios não localizados” ou “Fora de negócio”) não são úteis, e formam uma porcentagem bem pequena dos resultados de qualquer maneira."
      ],
      "metadata": {
        "id": "VoVaSZaO5gz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código a seguir é utilizado para converter nosso dataframe existente (df) em um novo dataframe em que cada inspeção é representada como um par de violações de rótulo. Nesse caso, um rótulo de 0.0 representa uma falha, um rótulo de 1.0 representa um sucesso e um rótulo de -1.0 representa alguns resultados diferentes desses dois."
      ],
      "metadata": {
        "id": "42G2TOWS55C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def labelForResults(s):\n",
        "    if s == 'Fail':\n",
        "        return 0.0\n",
        "    elif s == 'Pass w/ Conditions' or s == 'Pass':\n",
        "        return 1.0\n",
        "    else:\n",
        "        return -1.0\n",
        "label = UserDefinedFunction(labelForResults, DoubleType())\n",
        "labeledData = df.select(label(df.Results).alias('label'), df.Violations).where('label >= 0')"
      ],
      "metadata": {
        "id": "5vPDlMa54pUO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeledData.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MXn3PRc4waN",
        "outputId": "b64d92e5-04f6-463a-fac2-5f2cfdcd4121"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|          Violations|\n",
            "+-----+--------------------+\n",
            "|  0.0|18. NO EVIDENCE O...|\n",
            "|  1.0|                 NaN|\n",
            "|  0.0|18. NO EVIDENCE O...|\n",
            "|  1.0|41. PREMISES MAIN...|\n",
            "|  1.0|34. FLOORS: CONST...|\n",
            "|  0.0|2. FACILITIES TO ...|\n",
            "|  1.0|35. WALLS, CEILIN...|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "|  1.0|                 NaN|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l7RJ4bsh6Bp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeledData.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJE94w-V41pu",
        "outputId": "6dd5a0e2-4c8c-4080-98c3-891e5ef40040"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(label=0.0, Violations='18. NO EVIDENCE OF RODENT OR INSECT OUTER OPENINGS PROTECTED/RODENT PROOFED, A WRITTEN LOG SHALL BE MAINTAINED AVAILABLE TO THE INSPECTORS\\nComments:NO PEST CONTROL LOG BOOK ON SITE AT TIME OF INSPECTION, MUST PROVIDE, PREMISES OPEN FOR BUSINESS, AT THIS TIME,  SERIOUS CITATION ISSUED: 7-38-020 \\n33. FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS\\nComments:MUST REPLACE/CLEAN STAINED CUTTING IN PREP AREA \\n35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS\\nComments:MUST PROVIDE MISSING CEILING TILE REAR KITCHEN AREA, AND REPLACE STAINED CEILING TILE WHERE NEEDED \\n37. TOILET ROOM DOORS SELF CLOSING: DRESSING ROOMS WITH LOCKERS PROVIDED: COMPLETE SEPARATION FROM LIVING/SLEEPING QUARTERS\\nComments:SELF-CLOSING DEVICE NEEDED AT BATHROOM DOOR \\n38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED\\nComments:MUST PROVIDE THREE SINK STOPPERS FOR THREE COMPARTMENT SINK, REAR KITCHEN PREP AREA \\n44. ONLY AUTHORIZED PERSONNEL IN THE FOOD-PREP AREA\\nComments:MUST RESTRICT UNAUTHORIZED CUSTOMERS FROM COMING THROUGH FOOD PREP AREA, FROM BACK DOOR ')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando um modelo de regressão logística do dataframe de entrada"
      ],
      "metadata": {
        "id": "z0WBL5UV6CsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tarefa final é converter os dados rotulados. Converta os dados para um formato que possa ser analisado pela regressão logística. A entrada em um algoritmo de regressão logística precisa ser um conjunto de pares de vetor de recurso de rótulo. Em que o “vetor de recurso” é um vetor de números que representa o ponto de entrada. Portanto, você precisa converter a coluna “violações”, que é semiestruturada e contém vários comentários em texto livre. Converta a coluna para uma matriz de números reais que um computador entenderia facilmente.\n",
        "\n",
        "Uma abordagem de aprendizado de máquina padrão para o processamento de linguagem natural é atribuir um “índice” a cada palavra distinta. Depois, passe um vetor para o algoritmo de aprendizado de máquina. Faça isso de modo que o valor de cada índice contenha a frequência relativa dessa palavra na cadeia de texto.\n",
        "\n",
        "A MLlib proporciona uma forma fácil de executar essa operação. Primeiro, crie tokens de cada cadeia de caracteres de violações para obter as palavras individuais em cada cadeia de caracteres. Em seguida, use um HashingTF para converter cada conjunto de tokens em um vetor de recurso que pode ser passado para o algoritmo de regressão logística para construir um modelo. Você realiza todas essas etapas em sequência, usando um \"pipeline\"."
      ],
      "metadata": {
        "id": "oiQne1m36E7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"Violations\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "model = pipeline.fit(labeledData)"
      ],
      "metadata": {
        "id": "C26ajzVK6IcD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemso usar o modelo criado  para prever quais serão os resultados das novas inspeções. As previsões são baseadas nas violações que foram observadas.\n",
        "Esse modelo foi treinado pelo conjunto de dados Food_Inspections.csv.\n",
        "Podemos também usar um segundo conjunto de dados, Food_Inspections2.csv, para avaliar a força deste modelo nos dados novos.\n"
      ],
      "metadata": {
        "id": "_IetO9ic8vqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testData = sc.textFile('wasbs:///HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections2.csv')\\\n",
        "            .map(csvParse) \\\n",
        "            .map(lambda l: (int(l[0]), l[1], l[12], l[13]))\n",
        "testDf = spark.createDataFrame(testData, schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\n",
        "predictionsDf = model.transform(testDf)\n",
        "predictionsDf.registerTempTable('Predictions')\n",
        "predictionsDf.columns"
      ],
      "metadata": {
        "id": "mzxVoicf9N8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictionsDf.take(1)"
      ],
      "metadata": {
        "id": "cBtmDbV09ePf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método model.transform() aplica a mesma transformação para novos dados com o mesmo esquema e chega a uma previsão de como classificar os dados. Você pode realizar algumas estatísticas simples para ter uma ideia de quão precisas foram as previsões:"
      ],
      "metadata": {
        "id": "k-bh3KA19mrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numSuccesses = predictionsDf.where(\"\"\"(prediction = 0 AND results = 'Fail') OR\n",
        "                                        (prediction = 1 AND (results = 'Pass' OR\n",
        "                                                            results = 'Pass w/ Conditions'))\"\"\").count()\n",
        "numInspections = predictionsDf.count()\n",
        "\n",
        "print \"There were\", numInspections, \"inspections and there were\", numSuccesses, \"successful predictions\"\n",
        "print \"This is a\", str((float(numSuccesses) / float(numInspections)) * 100) + \"%\", \"success rate\""
      ],
      "metadata": {
        "id": "T1eVVfS09lnu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}